{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a02faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e12612f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>testimony</th>\n",
       "      <th>minutes</th>\n",
       "      <th>beige</th>\n",
       "      <th>speech1</th>\n",
       "      <th>speech2</th>\n",
       "      <th>btr</th>\n",
       "      <th>btc</th>\n",
       "      <th>inf</th>\n",
       "      <th>uem</th>\n",
       "      <th>itr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.962157</td>\n",
       "      <td>0.214615</td>\n",
       "      <td>0.016168</td>\n",
       "      <td>109.861229</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.22978</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.462258</td>\n",
       "      <td>0.994393</td>\n",
       "      <td>0.998924</td>\n",
       "      <td>0.142740</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>98.082925</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.14822</td>\n",
       "      <td>9.033333</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797096</td>\n",
       "      <td>0.998627</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.606771</td>\n",
       "      <td>0.370122</td>\n",
       "      <td>300.196282</td>\n",
       "      <td>16.1</td>\n",
       "      <td>3.34611</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>0.345389</td>\n",
       "      <td>-0.999317</td>\n",
       "      <td>-0.999372</td>\n",
       "      <td>0.460445</td>\n",
       "      <td>0.175324</td>\n",
       "      <td>-114.957873</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.71595</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>0.479481</td>\n",
       "      <td>0.994647</td>\n",
       "      <td>0.995748</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>-0.186537</td>\n",
       "      <td>-8.167803</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.34473</td>\n",
       "      <td>8.633333</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270508</td>\n",
       "      <td>0.998202</td>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.796364</td>\n",
       "      <td>0.395139</td>\n",
       "      <td>4.167270</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.82932</td>\n",
       "      <td>8.266667</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>0.662740</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.193265</td>\n",
       "      <td>-0.008554</td>\n",
       "      <td>31.287232</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.88792</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>0.932809</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>-0.997239</td>\n",
       "      <td>0.669289</td>\n",
       "      <td>0.356781</td>\n",
       "      <td>61.558895</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.68486</td>\n",
       "      <td>8.033333</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>-0.999350</td>\n",
       "      <td>0.150898</td>\n",
       "      <td>0.354380</td>\n",
       "      <td>0.024408</td>\n",
       "      <td>8.499321</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.90357</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321590</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.438148</td>\n",
       "      <td>-0.254101</td>\n",
       "      <td>192.990981</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.74020</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>0.599296</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.014922</td>\n",
       "      <td>-0.028363</td>\n",
       "      <td>0.061389</td>\n",
       "      <td>4.725288</td>\n",
       "      <td>97.5</td>\n",
       "      <td>1.41472</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015978</td>\n",
       "      <td>0.999497</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.688013</td>\n",
       "      <td>-0.109351</td>\n",
       "      <td>37.527021</td>\n",
       "      <td>141.9</td>\n",
       "      <td>1.50495</td>\n",
       "      <td>7.233333</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.010360</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.369943</td>\n",
       "      <td>0.221496</td>\n",
       "      <td>173.683708</td>\n",
       "      <td>805.9</td>\n",
       "      <td>1.20730</td>\n",
       "      <td>6.933333</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930044</td>\n",
       "      <td>0.999145</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0.697960</td>\n",
       "      <td>-0.088855</td>\n",
       "      <td>-59.455977</td>\n",
       "      <td>444.7</td>\n",
       "      <td>1.42991</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998851</td>\n",
       "      <td>0.990618</td>\n",
       "      <td>-0.995651</td>\n",
       "      <td>0.240892</td>\n",
       "      <td>-0.253543</td>\n",
       "      <td>35.638257</td>\n",
       "      <td>635.1</td>\n",
       "      <td>2.08034</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>0.432019</td>\n",
       "      <td>0.997272</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.034828</td>\n",
       "      <td>-0.004592</td>\n",
       "      <td>-49.226180</td>\n",
       "      <td>388.2</td>\n",
       "      <td>1.79097</td>\n",
       "      <td>6.066667</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>0.998091</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.263070</td>\n",
       "      <td>-0.160701</td>\n",
       "      <td>-19.884055</td>\n",
       "      <td>318.2</td>\n",
       "      <td>1.16401</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.325451</td>\n",
       "      <td>-0.961368</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.270814</td>\n",
       "      <td>0.245397</td>\n",
       "      <td>-26.510214</td>\n",
       "      <td>244.1</td>\n",
       "      <td>-0.11289</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>0.071191</td>\n",
       "      <td>0.990336</td>\n",
       "      <td>0.997860</td>\n",
       "      <td>0.502847</td>\n",
       "      <td>0.340416</td>\n",
       "      <td>7.874984</td>\n",
       "      <td>264.1</td>\n",
       "      <td>0.03701</td>\n",
       "      <td>5.433333</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>0.070047</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>0.458540</td>\n",
       "      <td>0.128990</td>\n",
       "      <td>-11.291983</td>\n",
       "      <td>235.9</td>\n",
       "      <td>0.15861</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007106</td>\n",
       "      <td>0.995270</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>0.329982</td>\n",
       "      <td>0.202621</td>\n",
       "      <td>60.037722</td>\n",
       "      <td>430.0</td>\n",
       "      <td>0.40047</td>\n",
       "      <td>5.033333</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998393</td>\n",
       "      <td>-0.049426</td>\n",
       "      <td>-0.085682</td>\n",
       "      <td>0.501187</td>\n",
       "      <td>-0.001863</td>\n",
       "      <td>-3.382136</td>\n",
       "      <td>415.7</td>\n",
       "      <td>0.99184</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>-0.128426</td>\n",
       "      <td>-0.425585</td>\n",
       "      <td>47.731387</td>\n",
       "      <td>670.0</td>\n",
       "      <td>1.11003</td>\n",
       "      <td>4.933333</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997880</td>\n",
       "      <td>-0.132664</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.947537</td>\n",
       "      <td>0.491948</td>\n",
       "      <td>-9.693837</td>\n",
       "      <td>608.1</td>\n",
       "      <td>1.15715</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.640552</td>\n",
       "      <td>0.992894</td>\n",
       "      <td>-0.995061</td>\n",
       "      <td>0.450028</td>\n",
       "      <td>-0.094828</td>\n",
       "      <td>46.012935</td>\n",
       "      <td>963.4</td>\n",
       "      <td>1.80698</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>0.805690</td>\n",
       "      <td>-0.995848</td>\n",
       "      <td>-0.128439</td>\n",
       "      <td>0.122847</td>\n",
       "      <td>11.341395</td>\n",
       "      <td>1079.1</td>\n",
       "      <td>2.58713</td>\n",
       "      <td>4.566667</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999670</td>\n",
       "      <td>0.994037</td>\n",
       "      <td>-0.994430</td>\n",
       "      <td>0.022070</td>\n",
       "      <td>0.661988</td>\n",
       "      <td>83.237311</td>\n",
       "      <td>2480.6</td>\n",
       "      <td>1.89059</td>\n",
       "      <td>4.366667</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998908</td>\n",
       "      <td>0.999412</td>\n",
       "      <td>-0.991948</td>\n",
       "      <td>-0.198641</td>\n",
       "      <td>0.595801</td>\n",
       "      <td>56.410920</td>\n",
       "      <td>4360.6</td>\n",
       "      <td>1.94494</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997013</td>\n",
       "      <td>0.869333</td>\n",
       "      <td>-0.987604</td>\n",
       "      <td>0.118023</td>\n",
       "      <td>0.119261</td>\n",
       "      <td>115.570445</td>\n",
       "      <td>13850.4</td>\n",
       "      <td>2.10778</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>-0.998887</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.247308</td>\n",
       "      <td>0.297858</td>\n",
       "      <td>-69.127174</td>\n",
       "      <td>6938.2</td>\n",
       "      <td>2.22100</td>\n",
       "      <td>4.033333</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999425</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>-0.994058</td>\n",
       "      <td>0.214136</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>-8.091627</td>\n",
       "      <td>6398.9</td>\n",
       "      <td>2.68761</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>0.879740</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>-0.332901</td>\n",
       "      <td>0.240191</td>\n",
       "      <td>3.626271</td>\n",
       "      <td>6635.2</td>\n",
       "      <td>2.64316</td>\n",
       "      <td>3.766667</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>0.996489</td>\n",
       "      <td>-0.874703</td>\n",
       "      <td>0.999586</td>\n",
       "      <td>0.684937</td>\n",
       "      <td>0.320146</td>\n",
       "      <td>-58.151867</td>\n",
       "      <td>3709.4</td>\n",
       "      <td>2.20502</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998110</td>\n",
       "      <td>-0.938936</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>0.696936</td>\n",
       "      <td>0.424436</td>\n",
       "      <td>10.067765</td>\n",
       "      <td>4102.3</td>\n",
       "      <td>1.62017</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998962</td>\n",
       "      <td>0.997378</td>\n",
       "      <td>-0.978499</td>\n",
       "      <td>0.172317</td>\n",
       "      <td>0.086671</td>\n",
       "      <td>96.971908</td>\n",
       "      <td>10818.6</td>\n",
       "      <td>1.83580</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.332461</td>\n",
       "      <td>-0.000599</td>\n",
       "      <td>-0.010144</td>\n",
       "      <td>-0.127286</td>\n",
       "      <td>0.149723</td>\n",
       "      <td>-26.690472</td>\n",
       "      <td>8284.3</td>\n",
       "      <td>1.76934</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.319638</td>\n",
       "      <td>-0.991490</td>\n",
       "      <td>-0.010144</td>\n",
       "      <td>0.324815</td>\n",
       "      <td>0.303295</td>\n",
       "      <td>-14.078126</td>\n",
       "      <td>7196.4</td>\n",
       "      <td>2.01644</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999031</td>\n",
       "      <td>0.083495</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.090842</td>\n",
       "      <td>-0.047369</td>\n",
       "      <td>-11.533169</td>\n",
       "      <td>6412.5</td>\n",
       "      <td>2.10244</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0.597694</td>\n",
       "      <td>-0.999171</td>\n",
       "      <td>-0.994271</td>\n",
       "      <td>0.545493</td>\n",
       "      <td>0.553235</td>\n",
       "      <td>35.390777</td>\n",
       "      <td>9135.4</td>\n",
       "      <td>0.44474</td>\n",
       "      <td>12.966667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.278174</td>\n",
       "      <td>0.997787</td>\n",
       "      <td>-0.996841</td>\n",
       "      <td>0.229175</td>\n",
       "      <td>-0.028742</td>\n",
       "      <td>16.517374</td>\n",
       "      <td>10776.1</td>\n",
       "      <td>1.25385</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.160328</td>\n",
       "      <td>-0.948996</td>\n",
       "      <td>-0.995986</td>\n",
       "      <td>0.440566</td>\n",
       "      <td>0.104101</td>\n",
       "      <td>98.821876</td>\n",
       "      <td>28949.4</td>\n",
       "      <td>1.20009</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.957833</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.536052</td>\n",
       "      <td>-0.216787</td>\n",
       "      <td>70.797484</td>\n",
       "      <td>58763.7</td>\n",
       "      <td>1.89726</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333228</td>\n",
       "      <td>0.865968</td>\n",
       "      <td>0.999489</td>\n",
       "      <td>0.497726</td>\n",
       "      <td>0.473530</td>\n",
       "      <td>-51.740798</td>\n",
       "      <td>35026.9</td>\n",
       "      <td>4.81323</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.018144</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.590058</td>\n",
       "      <td>0.412583</td>\n",
       "      <td>22.404930</td>\n",
       "      <td>43823.3</td>\n",
       "      <td>5.29055</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999534</td>\n",
       "      <td>0.992108</td>\n",
       "      <td>-0.987153</td>\n",
       "      <td>0.310546</td>\n",
       "      <td>-0.071408</td>\n",
       "      <td>5.323615</td>\n",
       "      <td>46219.5</td>\n",
       "      <td>6.72125</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996522</td>\n",
       "      <td>0.024958</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.581608</td>\n",
       "      <td>0.589203</td>\n",
       "      <td>-1.514016</td>\n",
       "      <td>45525.0</td>\n",
       "      <td>8.00039</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  year  quarter  testimony   minutes     beige   speech1  \\\n",
       "0            1  2010        4   0.006298  0.999464  0.962157  0.214615   \n",
       "1            2  2011        1  -0.462258  0.994393  0.998924  0.142740   \n",
       "2            3  2011        2   0.797096  0.998627  0.027240  0.606771   \n",
       "3            4  2011        3   0.345389 -0.999317 -0.999372  0.460445   \n",
       "4            5  2011        4   0.479481  0.994647  0.995748  0.001555   \n",
       "5            6  2012        1   0.270508  0.998202  0.019599  0.796364   \n",
       "6            7  2012        2   0.662740  0.999127  0.999800  0.193265   \n",
       "7            8  2012        3   0.932809  0.000188 -0.997239  0.669289   \n",
       "8            9  2012        4   0.998409 -0.999350  0.150898  0.354380   \n",
       "9           10  2013        1   0.321590  0.000007  0.000078  0.438148   \n",
       "10          11  2013        2   0.599296 -0.000257  0.014922 -0.028363   \n",
       "11          12  2013        3   0.015978  0.999497  0.016791  0.688013   \n",
       "12          13  2013        4   0.999745  0.010360  0.000477  0.369943   \n",
       "13          14  2014        1   0.930044  0.999145  0.999742  0.697960   \n",
       "14          15  2014        2   0.998851  0.990618 -0.995651  0.240892   \n",
       "15          16  2014        3   0.432019  0.997272  0.007932  0.034828   \n",
       "16          17  2014        4   0.008891  0.998091  0.999852  0.263070   \n",
       "17          18  2015        1  -0.325451 -0.961368  0.999638  0.270814   \n",
       "18          19  2015        2   0.071191  0.990336  0.997860  0.502847   \n",
       "19          20  2015        3   0.070047  0.135900  0.999010  0.458540   \n",
       "20          21  2015        4   0.007106  0.995270 -0.000701  0.329982   \n",
       "21          22  2016        1   0.998393 -0.049426 -0.085682  0.501187   \n",
       "22          23  2016        2   0.002068  0.004305 -0.002168 -0.128426   \n",
       "23          24  2016        3   0.997880 -0.132664  0.003493  0.947537   \n",
       "24          25  2016        4  -0.640552  0.992894 -0.995061  0.450028   \n",
       "25          26  2017        1   0.999466  0.805690 -0.995848 -0.128439   \n",
       "26          27  2017        2   0.999670  0.994037 -0.994430  0.022070   \n",
       "27          28  2017        3   0.998908  0.999412 -0.991948 -0.198641   \n",
       "28          29  2017        4   0.997013  0.869333 -0.987604  0.118023   \n",
       "29          30  2018        1   0.999529 -0.998887  0.002555  0.247308   \n",
       "30          31  2018        2   0.999425  0.000394 -0.994058  0.214136   \n",
       "31          32  2018        3   0.879740  0.002403  0.002940 -0.332901   \n",
       "32          33  2018        4   0.996489 -0.874703  0.999586  0.684937   \n",
       "33          34  2019        1   0.998110 -0.938936 -0.000215  0.696936   \n",
       "34          35  2019        2   0.998962  0.997378 -0.978499  0.172317   \n",
       "35          36  2019        3  -0.332461 -0.000599 -0.010144 -0.127286   \n",
       "36          37  2019        4   0.319638 -0.991490 -0.010144  0.324815   \n",
       "37          38  2020        1   0.999031  0.083495  0.002681  0.090842   \n",
       "38          39  2020        2   0.597694 -0.999171 -0.994271  0.545493   \n",
       "39          40  2020        3  -0.278174  0.997787 -0.996841  0.229175   \n",
       "40          41  2020        4  -0.160328 -0.948996 -0.995986  0.440566   \n",
       "41          42  2021        1   0.002008  0.957833  0.001776  0.536052   \n",
       "42          43  2021        2   0.333228  0.865968  0.999489  0.497726   \n",
       "43          44  2021        3   0.999663  0.018144  0.999849  0.590058   \n",
       "44          45  2021        4   0.999534  0.992108 -0.987153  0.310546   \n",
       "45          46  2022        1   0.996522  0.024958  0.003733  0.581608   \n",
       "\n",
       "     speech2         btr      btc      inf        uem   itr  label  \n",
       "0   0.016168  109.861229      0.3  1.22978   9.500000  0.19    0.0  \n",
       "1   0.138890   98.082925      0.8  2.14822   9.033333  0.17    0.0  \n",
       "2   0.370122  300.196282     16.1  3.34611   9.066667  0.10    0.0  \n",
       "3   0.175324 -114.957873      5.1  3.71595   9.000000  0.07    0.0  \n",
       "4  -0.186537   -8.167803      4.7  3.34473   8.633333  0.07    0.0  \n",
       "5   0.395139    4.167270      4.9  2.82932   8.266667  0.08    1.0  \n",
       "6  -0.008554   31.287232      6.7  1.88792   8.200000  0.14    1.0  \n",
       "7   0.356781   61.558895     12.4  1.68486   8.033333  0.16    0.0  \n",
       "8   0.024408    8.499321     13.5  1.90357   7.800000  0.16    0.0  \n",
       "9  -0.254101  192.990981     93.0  1.74020   7.733333  0.14    0.0  \n",
       "10  0.061389    4.725288     97.5  1.41472   7.533333  0.15    0.0  \n",
       "11 -0.109351   37.527021    141.9  1.50495   7.233333  0.09    0.0  \n",
       "12  0.221496  173.683708    805.9  1.20730   6.933333  0.09    0.0  \n",
       "13 -0.088855  -59.455977    444.7  1.42991   6.666667  0.07    0.0  \n",
       "14 -0.253543   35.638257    635.1  2.08034   6.200000  0.09    1.0  \n",
       "15 -0.004592  -49.226180    388.2  1.79097   6.066667  0.09    0.0  \n",
       "16 -0.160701  -19.884055    318.2  1.16401   5.700000  0.09    1.0  \n",
       "17  0.245397  -26.510214    244.1 -0.11289   5.533333  0.11    0.0  \n",
       "18  0.340416    7.874984    264.1  0.03701   5.433333  0.12    1.0  \n",
       "19  0.128990  -11.291983    235.9  0.15861   5.100000  0.13    0.0  \n",
       "20  0.202621   60.037722    430.0  0.40047   5.033333  0.12    1.0  \n",
       "21 -0.001863   -3.382136    415.7  0.99184   4.900000  0.34    1.0  \n",
       "22 -0.425585   47.731387    670.0  1.11003   4.933333  0.37    1.0  \n",
       "23  0.491948   -9.693837    608.1  1.15715   4.900000  0.39    1.0  \n",
       "24 -0.094828   46.012935    963.4  1.80698   4.766667  0.40    1.0  \n",
       "25  0.122847   11.341395   1079.1  2.58713   4.566667  0.65    1.0  \n",
       "26  0.661988   83.237311   2480.6  1.89059   4.366667  0.90    1.0  \n",
       "27  0.595801   56.410920   4360.6  1.94494   4.333333  1.15    1.0  \n",
       "28  0.119261  115.570445  13850.4  2.10778   4.166667  1.15    1.0  \n",
       "29  0.297858  -69.127174   6938.2  2.22100   4.033333  1.41    1.0  \n",
       "30  0.004461   -8.091627   6398.9  2.68761   3.933333  1.69    1.0  \n",
       "31  0.240191    3.626271   6635.2  2.64316   3.766667  1.91    1.0  \n",
       "32  0.320146  -58.151867   3709.4  2.20502   3.833333  2.19    1.0  \n",
       "33  0.424436   10.067765   4102.3  1.62017   3.866667  2.40    1.0  \n",
       "34  0.086671   96.971908  10818.6  1.83580   3.600000  2.42    0.0  \n",
       "35  0.149723  -26.690472   8284.3  1.76934   3.633333  2.40    0.0  \n",
       "36  0.303295  -14.078126   7196.4  2.01644   3.600000  1.83    0.0  \n",
       "37 -0.047369  -11.533169   6412.5  2.10244   3.800000  1.55    0.0  \n",
       "38  0.553235   35.390777   9135.4  0.44474  12.966667  0.05    0.0  \n",
       "39 -0.028742   16.517374  10776.1  1.25385   8.833333  0.09    0.0  \n",
       "40  0.104101   98.821876  28949.4  1.20009   6.766667  0.09    0.0  \n",
       "41 -0.216787   70.797484  58763.7  1.89726   6.200000  0.09    0.0  \n",
       "42  0.473530  -51.740798  35026.9  4.81323   5.900000  0.07    0.0  \n",
       "43  0.412583   22.404930  43823.3  5.29055   5.100000  0.10    0.0  \n",
       "44 -0.071408    5.323615  46219.5  6.72125   4.233333  0.08    0.0  \n",
       "45  0.589203   -1.514016  45525.0  8.00039   3.800000  0.08    1.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../data/df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bae240",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['testimony','minutes','beige','speech1','speech2']].to_numpy()\n",
    "label = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47456772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 0.4000\n"
     ]
    }
   ],
   "source": [
    "df_clf = DecisionTreeClassifier(random_state = 10)\n",
    "df_clf.fit(x_train, y_train)\n",
    "pred = df_clf.predict(x_test)\n",
    "\n",
    "print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5da2701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도 : [0.4375 0.5333 0.5333]\n",
      "평균 검증 정확도 : 0.5014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "dt_clf = DecisionTreeClassifier(random_state = 10)\n",
    "\n",
    "scores = cross_val_score(dt_clf, train, label, scoring='accuracy', cv = 3)\n",
    "print('교차 검증별 정확도 :', np.round(scores, 4))\n",
    "print('평균 검증 정확도 :', np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08305417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe08fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting 분류기 정확도 : 0.7000\n",
      "LogisticRegression 정확도: 0.6000\n",
      "KNeighborsClassifier 정확도: 0.6000\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(random_state = 10)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "vo_clf = VotingClassifier(estimators=[('LR', lr_clf),('KNN', knn_clf)], voting = 'soft')\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)\n",
    "\n",
    "vo_clf.fit(x_train, y_train)\n",
    "pred = vo_clf.predict(x_test)\n",
    "print('Voting 분류기 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(x_train, y_train)\n",
    "    pred = classifier.predict(x_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c356abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도 : 0.5000\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 0)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "pred = rf_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print('랜덤 포레스트 정확도 : {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9a2544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.600\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(x_train, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(x_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "255bed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼동행렬, 정확도, 정밀도, 재현율, F1, AUC 불러오기\n",
    "def get_clf_eval(y_test, y_pred):\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred)\n",
    "    AUC = roc_auc_score(y_test, y_pred)\n",
    "    print('오차행렬:\\n', confusion)\n",
    "    print('\\n정확도: {:.4f}'.format(accuracy))\n",
    "    print('정밀도: {:.4f}'.format(precision))\n",
    "    print('재현율: {:.4f}'.format(recall))\n",
    "    print('F1: {:.4f}'.format(F1))\n",
    "    print('AUC: {:.4f}'.format(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32bb76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "오차행렬:\n",
      " [[5 2]\n",
      " [1 2]]\n",
      "\n",
      "정확도: 0.7000\n",
      "정밀도: 0.5000\n",
      "재현율: 0.6667\n",
      "F1: 0.5714\n",
      "AUC: 0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuseonjong/miniforge3/envs/tf/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators = 400, learning_rate = 0.1, max_depth = 3)\n",
    "xgb_wrapper.fit(x_train, y_train)\n",
    "w_preds = xgb_wrapper.predict(x_test)\n",
    "\n",
    "get_clf_eval(y_test, w_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2784e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['inf','uem']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85a03fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 0.7000\n",
      "교차 검증별 정확도 : [0.625  0.8667 0.5333]\n",
      "평균 검증 정확도 : 0.675\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)\n",
    "\n",
    "df_clf = DecisionTreeClassifier(random_state = 10)\n",
    "df_clf.fit(x_train, y_train)\n",
    "pred = df_clf.predict(x_test)\n",
    "\n",
    "print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dt_clf = DecisionTreeClassifier(random_state = 10)\n",
    "\n",
    "scores = cross_val_score(dt_clf, train, label, scoring='accuracy', cv = 3)\n",
    "print('교차 검증별 정확도 :', np.round(scores, 4))\n",
    "print('평균 검증 정확도 :', np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c27638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting 분류기 정확도 : 0.8000\n",
      "LogisticRegression 정확도: 0.8000\n",
      "KNeighborsClassifier 정확도: 0.7000\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(random_state = 10)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "vo_clf = VotingClassifier(estimators=[('LR', lr_clf),('KNN', knn_clf)], voting = 'soft')\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)\n",
    "\n",
    "vo_clf.fit(x_train, y_train)\n",
    "pred = vo_clf.predict(x_test)\n",
    "print('Voting 분류기 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(x_train, y_train)\n",
    "    pred = classifier.predict(x_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c198603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도 : 0.9000\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 0)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "pred = rf_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print('랜덤 포레스트 정확도 : {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe5ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.700\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(x_train, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(x_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71ae54b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:43:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "오차행렬:\n",
      " [[4 3]\n",
      " [1 2]]\n",
      "\n",
      "정확도: 0.6000\n",
      "정밀도: 0.4000\n",
      "재현율: 0.6667\n",
      "F1: 0.5000\n",
      "AUC: 0.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuseonjong/miniforge3/envs/tf/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators = 400, learning_rate = 0.1, max_depth = 3)\n",
    "xgb_wrapper.fit(x_train, y_train)\n",
    "w_preds = xgb_wrapper.predict(x_test)\n",
    "\n",
    "get_clf_eval(y_test, w_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0c33041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[['testimony','minutes','beige','speech1','speech2', 'inf','uem']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "600175ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 0.8000\n",
      "교차 검증별 정확도 : [0.625  0.4    0.4667]\n",
      "평균 검증 정확도 : 0.4972\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)\n",
    "\n",
    "df_clf = DecisionTreeClassifier(random_state = 10)\n",
    "df_clf.fit(x_train, y_train)\n",
    "pred = df_clf.predict(x_test)\n",
    "\n",
    "print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "dt_clf = DecisionTreeClassifier(random_state = 10)\n",
    "\n",
    "scores = cross_val_score(dt_clf, train, label, scoring='accuracy', cv = 3)\n",
    "print('교차 검증별 정확도 :', np.round(scores, 4))\n",
    "print('평균 검증 정확도 :', np.round(np.mean(scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b4d05d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting 분류기 정확도 : 0.8000\n",
      "LogisticRegression 정확도: 0.8000\n",
      "KNeighborsClassifier 정확도: 0.7000\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(random_state = 10)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "vo_clf = VotingClassifier(estimators=[('LR', lr_clf),('KNN', knn_clf)], voting = 'soft')\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, label, test_size = 0.2, random_state = 10)\n",
    "\n",
    "vo_clf.fit(x_train, y_train)\n",
    "pred = vo_clf.predict(x_test)\n",
    "print('Voting 분류기 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))\n",
    "\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(x_train, y_train)\n",
    "    pred = classifier.predict(x_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef2be193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도 : 0.7000\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(random_state = 0)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "pred = rf_clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print('랜덤 포레스트 정확도 : {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9fdeb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 1.000\n",
      "테스트 세트 정확도: 0.900\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(x_train, y_train)\n",
    "\n",
    "print(\"훈련 세트 정확도: {:.3f}\".format(gbrt.score(x_train, y_train)))\n",
    "print(\"테스트 세트 정확도: {:.3f}\".format(gbrt.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f6b2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 60.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuseonjong/miniforge3/envs/tf/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09c9d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:44:02] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "오차행렬:\n",
      " [[5 2]\n",
      " [1 2]]\n",
      "\n",
      "정확도: 0.7000\n",
      "정밀도: 0.5000\n",
      "재현율: 0.6667\n",
      "F1: 0.5714\n",
      "AUC: 0.6905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuseonjong/miniforge3/envs/tf/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators = 400, learning_rate = 0.1, max_depth = 3)\n",
    "xgb_wrapper.fit(x_train, y_train)\n",
    "w_preds = xgb_wrapper.predict(x_test)\n",
    "\n",
    "get_clf_eval(y_test, w_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1e8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
